{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
        "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\""
      ],
      "metadata": {
        "id": "REoyMKJjnwHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah-NEu7pm7Dx"
      },
      "outputs": [],
      "source": [
        "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
        "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
        "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
        "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
        "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
        "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
        "LLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"persist_directory\":None,\n",
        "          \"load_in_8bit\":False,\n",
        "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
        "          \"llm\":LLM_FLAN_T5_BASE,\n",
        "          }"
      ],
      "metadata": {
        "id": "sZsyUriLnn9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain\n"
      ],
      "metadata": {
        "id": "GOmkr7UJolGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentence-transformers"
      ],
      "metadata": {
        "id": "mMJjTvrVx0mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n"
      ],
      "metadata": {
        "id": "ooum2RQe0Vi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate"
      ],
      "metadata": {
        "id": "oicq84_f1Nn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from accelerate import Accelerator\n",
        "\n",
        "\n",
        "def create_sbert_mpnet():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
        "\n",
        "def create_flan_t5_base(load_in_8bit=False):\n",
        "    # Wrap it in HF pipeline for use with LangChain\n",
        "    model = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    return pipeline(\n",
        "        task=\"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=100,\n",
        "        model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
        "    )\n",
        "\n",
        "# Assuming EMB_SBERT_MPNET_BASE and LLM_FLAN_T5_BASE are defined elsewhere in your code\n",
        "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
        "    embedding = create_sbert_mpnet()\n",
        "\n",
        "load_in_8bit = config[\"load_in_8bit\"]\n",
        "\n",
        "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
        "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awvdDHCDnrV4",
        "outputId": "c3fb460d-f372-4968-aa3d-a7f66f7719ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_falcon_instruct_small(load_in_8bit=False):\n",
        "        model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        hf_pipeline = pipeline(\n",
        "                task=\"text-generation\",\n",
        "                model = model,\n",
        "                tokenizer = tokenizer,\n",
        "                trust_remote_code = True,\n",
        "                max_new_tokens=100,\n",
        "                model_kwargs={\n",
        "                    \"device_map\": \"auto\",\n",
        "                    \"load_in_8bit\": load_in_8bit,\n",
        "                    \"max_length\": 512,\n",
        "                    \"temperature\": 0.01,\n",
        "                    \"torch_dtype\":torch.bfloat16,\n",
        "                    }\n",
        "            )\n",
        "        return hf_pipeline"
      ],
      "metadata": {
        "id": "J7T5_jd62Eap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdfplumber"
      ],
      "metadata": {
        "id": "5GHZIuzN2wy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "5xMM50fIL5nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "pdf_path = \"/content/first_pdf.pdf\"\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    # Extract text from each page\n",
        "    documents = [page.extract_text() for page in pdf.pages]\n",
        "\n",
        "# Split documents into text snippets (directly pass the text strings)\n",
        "chunk_size = 100\n",
        "chunk_overlap = 0\n",
        "texts = []\n",
        "\n",
        "for document in documents:\n",
        "    start = 0\n",
        "    while start < len(document):\n",
        "        end = start + chunk_size\n",
        "        texts.append(document[start:end])\n",
        "        start = end - chunk_overlap\n",
        "\n"
      ],
      "metadata": {
        "id": "woryXwkb2EdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install chromadb"
      ],
      "metadata": {
        "id": "vGbZs80lO188"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.chroma import Chroma\n",
        "\n",
        "# Assuming 'embedding' is already defined\n",
        "persist_directory = config[\"persist_directory\"]\n",
        "\n",
        "# Create a custom Document class with 'page_content' and 'metadata' attributes\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "# Assuming 'texts' is a list of text extracted from each page\n",
        "# You can provide metadata as needed for each document\n",
        "documents = [Document(page_content=text, metadata={'some_key': 'some_value'}) for text in texts]\n",
        "\n",
        "# Use Chroma.from_documents with the custom Document objects\n",
        "vectordb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=persist_directory)\n"
      ],
      "metadata": {
        "id": "TZKF4fAUN0Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
        "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
        "\n",
        "# Defining a default prompt for flan models\n",
        "if config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
        "    question_t5_template = \"\"\"\n",
        "    context: {context}\n",
        "    question: {question}\n",
        "    answer:\n",
        "    \"\"\"\n",
        "    QUESTION_T5_PROMPT = PromptTemplate(\n",
        "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT"
      ],
      "metadata": {
        "id": "iAKsZT7rN0T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "question = \": I cry\"\n",
        "qa.combine_documents_chain.verbose = True\n",
        "qa.return_source_documents = True\n",
        "qa({\"query\":question,})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge8IxtbTN0X1",
        "outputId": "6e5b8dd8-f16c-4889-b9b6-8f5cb8cec064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': ': I cry',\n",
              " 'result': 'THERAPIST: I cry',\n",
              " 'source_documents': [Document(page_content=\"CLIENT 3: Uh well my sister and I don't get a long at all We just sit\\nthere and fight and throw insu\", metadata={'some_key': 'some_value'}),\n",
              "  Document(page_content='n\\neverybody else\\nCLIENT 1: laughs', metadata={'some_key': 'some_value'}),\n",
              "  Document(page_content='CLIENT 2: laughs', metadata={'some_key': 'some_value'}),\n",
              "  Document(page_content='le 00 23 12\\nTHERAPIST: Why why is there so much uhm bitterness between you and her\\nCLIENT 3: I think', metadata={'some_key': 'some_value'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAykOGA_N0br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxX_DSNRN0j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtZcWRIGN0o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00jNErDnN0s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34D2cZHwN0vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-T5nlmON0y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "\n",
        "print(dir(langchain))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvvX6Fzs31yx",
        "outputId": "855427ae-2f81-4861-db32-2b38376324a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Any', 'Optional', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__getattr__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_warn_on_import', 'embeddings', 'storage', 'surface_langchain_deprecation_warnings', 'text_splitter', 'utils', 'vectorstores', 'warnings']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "\n",
        "print(dir(langchain))\n"
      ],
      "metadata": {
        "id": "xwBdM-bb2Eg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3y1SDwO2Ejd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejbpjew52EnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMVyla9B2Eps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this code find the path of hugging face embeddigns\n",
        "# import os\n",
        "\n",
        "# langchain_path = '/usr/local/lib/python3.10/dist-packages/langchain'\n",
        "# target_class_name = 'HuggingFaceEmbeddings'\n",
        "\n",
        "# for root, dirs, files in os.walk(langchain_path):\n",
        "#     for file in files:\n",
        "#         if file.endswith(\".py\"):\n",
        "#             file_path = os.path.join(root, file)\n",
        "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#                 contents = f.read()\n",
        "#                 if target_class_name in contents:\n",
        "#                     relative_path = file_path.replace(langchain_path, 'langchain')\n",
        "#                     print(f'Found in: {relative_path}')\n"
      ],
      "metadata": {
        "id": "TptKgDRZn3lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade langchain"
      ],
      "metadata": {
        "id": "uZ14_oI2oKXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQXGg7W2pMbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}